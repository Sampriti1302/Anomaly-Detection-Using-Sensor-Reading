# ğŸš¨ Sensor Anomaly Detection System | 79% F1 Score on 116:1 Imbalanced Data

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![XGBoost](https://img.shields.io/badge/XGBoost-Latest-red.svg)](https://xgboost.readthedocs.io/)


## ğŸ¯ Project Highlights

- **ğŸ”¥ 79.2% F1 Score** - Achieved top-tier performance on highly imbalanced anomaly detection (99.14% normal vs 0.86% anomaly)
- **ğŸ“Š 1.6M+ Records** - Processed large-scale time-series sensor data with extreme outliers (values up to 10^38)
- **âš¡ 216% Improvement** - Boosted model performance from 0.25 baseline to 0.79 F1 through innovative feature engineering
- **ğŸ“ 92% Precision** - Minimized false positives with data-driven threshold optimization

---

## ğŸš€ Key Achievements

| Metric | Value | Description |
|--------|-------|-------------|
| **F1 Score** | 0.7919 | Balanced precision-recall performance |
| **Precision** | 0.9199 | 92% of predicted anomalies are correct |
| **Recall** | 0.6952 | Catches 70% of all anomalies |
| **ROC-AUC** | 0.9937 | Near-perfect discrimination ability |

---

## ğŸ› ï¸ Technical Stack

**Languages & Libraries:**
- Python 3.8+ | NumPy | Pandas | Scikit-learn

**Machine Learning Models:**
- Gradient Boosting: XGBoost, LightGBM, CatBoost
- Ensemble Methods: Random Forest
- Classical ML: Logistic Regression, Decision Tree, KNN

**Data Processing:**
- Feature Engineering (29 custom features)
- Time-series Analysis
- Imbalanced Data Handling
- Outlier Detection & Treatment

---

## ğŸ“ˆ Project Workflow
```
Raw Sensor Data (1.6M rows Ã— 7 features)
    â†“
Exploratory Data Analysis
    â†“
Feature Engineering (29 new features)
    â”œâ”€ Temporal Features (year, month, day, cyclical encoding)
    â”œâ”€ Outlier Detection (quantile-based capping at 99th percentile)
    â”œâ”€ Log Transformations (handle extreme scales)
    â”œâ”€ Interaction Features (sensor relationships)
    â”œâ”€ Statistical Features (mean, std, min, max)
    â””â”€ Binary Flags (zero detection, threshold-based)
    â†“
Model Training & Evaluation (7 models)
    â”œâ”€ Classical: Logistic Regression, Decision Tree, KNN
    â””â”€ Advanced: Random Forest, XGBoost, LightGBM, CatBoost
    â†“
Best Model Selection (XGBoost: F1 = 0.79)
    â†“
Final Predictions & Deployment
```

---

## ğŸ¨ Feature Engineering Innovation

### ğŸ”‘ **Game-Changer: Adaptive Quantile-Based Capping**
```python
# Instead of arbitrary thresholds (e.g., clip at 100)
# Used data-driven 99th percentile capping
x3_cap = df['X3'].quantile(0.99)
x4_cap = df['X4'].quantile(0.99)

df['X3_capped'] = df['X3'].clip(upper=x3_cap)
df['X4_capped'] = df['X4'].clip(upper=x4_cap)
```

**Impact:** 
- âœ… Prevented infinity values in interaction features
- âœ… Preserved anomaly signal while controlling extremes
- âœ… Improved XGBoost precision from 30% â†’ 92%

### ğŸ“Š **29 Engineered Features Created:**

1. **Temporal Features (9):** year, month, day, dayofweek, quarter, dayofyear, weekofyear, cyclical encoding
2. **Outlier Detection (2):** X3_capped, X4_capped (99th percentile)
3. **Transformations (2):** X3_log, X4_log
4. **Interaction Features (5):** X1_X2, X1_X5, X3_X4, ratios, divisions
5. **Statistical Features (7):** sum, mean, std, min, max
6. **Polynomial Features (2):** X1_squared, X5_squared
7. **Binary Flags (3):** X5_is_zero, X3_is_one, X4_is_one

---

## ğŸ† Model Performance Comparison

| Model | F1 Score | Precision | Recall | ROC-AUC |
|-------|----------|-----------|--------|---------|
| **XGBoost** ğŸ¥‡ | **0.7919** | **0.9199** | 0.6952 | **0.9937** |
| LightGBM ğŸ¥ˆ | 0.7682 | 0.9127 | 0.6631 | 0.9928 |
| CatBoost ğŸ¥‰ | 0.7601 | 0.9209 | 0.6471 | 0.9919 |
| Random Forest | 0.6914 | 0.9124 | 0.5566 | 0.9907 |
| KNN | 0.4656 | 0.6803 | 0.3426 | 0.8552 |
| Decision Tree | 0.2498 | 0.1440 | 0.9405 | 0.9780 |
| Logistic Regression | 0.2446 | 0.1407 | 0.9352 | 0.9853 |

---

## ğŸ“ Project Structure
```
anomaly-detection-sensor/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.parquet              # Training dataset (1.6M rows)
â”‚   â”œâ”€â”€ test.parquet               # Test dataset (410K rows)
â”‚   â””â”€â”€ submission.parquet         # Final predictions
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ anaverse_anomaly_detection.ipynb  # Complete analysis & modeling
â”‚
â”œâ”€â”€ requirements.txt               # Dependencies
â”‚
â””â”€â”€ README.md                      # This fil#
```

---

## ğŸš€ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/Sampriti1302/Anomaly-Detection-Using-Sensor-Reading.git
cd anomaly-detection-sensor

# Install dependencies
pip install -r requirements.txt
```

### Usage
```python
# Load and preprocess data
from src.feature_engineering import create_features
train_processed = create_features(train, is_train=True)

# Train model
from src.model_training import train_xgboost
model = train_xgboost(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
```

---

## ğŸ“Š Key Insights

### ğŸ” **Data Characteristics**
- **Highly Imbalanced:** 1,625,386 normal vs 14,038 anomalies (116:1 ratio)
- **Extreme Outliers:** X3 and X4 features with values up to 10^38
- **Time-Series:** 4 years of sensor readings (2020-2024)
- **Clean Data:** No missing values

### ğŸ’¡ **Critical Findings**
1. **Extreme values correlated with anomalies** - 99th percentile capping preserved signal
2. **Tree-based models excel** - Gradient boosting achieved 92% precision
3. **Feature engineering critical** - 29 features improved F1 by 216%
4. **Quantile-based thresholds** - Data-driven approach beat arbitrary limits

---

## ğŸ“ Lessons Learned

âœ… **Adaptive thresholds** (quantile-based) > Fixed thresholds
âœ… **Safe feature engineering** prevents infinity/NaN issues
âœ… **Regularization crucial** for large datasets (1.6M+ rows)
âœ… **Domain knowledge** (sensor behavior) guides feature creation
âœ… **Precision-recall trade-off** matters in imbalanced scenarios

---

## ğŸ”® Future Enhancements

- [ ] **Threshold Tuning:** Optimize decision boundary for F1 â†’ 0.82+
- [ ] **Ensemble Methods:** Combine XGBoost + LightGBM + CatBoost
- [ ] **SMOTE:** Synthetic minority oversampling
- [ ] **Optuna:** Automated hyperparameter optimization
- [ ] **Deep Learning:** LSTM for time-series patterns
- [ ] **Real-time Inference:** Deploy model as REST API

---

## ğŸ“ Citation

If you use this project in your research or work, please cite:
```bibtex
@project{anomaly_detection_sensor_2026,
  title={Sensor Anomaly Detection System with 79% F1 Score},
  author={Sampriti Mahato},
  year={2026},
  url={https://github.com/Sampriti1302/Anomaly-Detection-Using-Sensor-Reading}
}
```

## ğŸ™ Acknowledgments

- **Celebal Technologies** - For organizing AnaVerse 2.0 Data Science Challenge
- **Kaggle Community** - For datasets and inspiration
- **Open Source Libraries** - scikit-learn, XGBoost, LightGBM, CatBoost

---


<div align="center">
  
### â­ Star this repo if you found it helpful!

**Built with â¤ï¸ and lots of â˜•**

</div>
```
